## MultiMNIST

The MultiMNIST dataset, originally proposed by Dimitriadis et al. [[2]](#2), is a multi-task learning benchmark dataset created by overlaying pairs of MNIST digits [[1]](#1) to create a multi-digit classification problem. Each image contains two overlapping digits, and the task is to classify both digits correctly. This creates a natural multi-task learning scenario where the model needs to learn to recognize both digits simultaneously.

The dataset is generated by randomly sampling pairs of MNIST digits and overlaying them with random translations and rotations. Each input image has been resized to <img src="https://render.githubusercontent.com/render/math?math=1\times36\times36"> and has labels for two tasks: classifying the first digit and classifying the second digit. Thus, it is a single-input problem, which means ``multi_input`` must be ``False``.

The training codes are based on a shared encoder architecture with task-specific heads. We use a LeNet-based architecture with shared convolutional layers and separate fully connected layers for each task.

The evaluation metric for all tasks is classification accuracy, where higher scores indicate better performance.

### Dataset Options

1. **MultiMNIST (2 tasks)**
   - Task 1: Classify the top-left digit
   - Task 2: Classify the bottom-right digit

2. **MultiMNIST-3 (3 tasks)**
   - Task 1: Classify the top-left digit
   - Task 2: Classify the bottom-center digit
   - Task 3: Classify the top-right digit

### Run a Model

The script ``main.py`` is the main file for training and evaluating an MTL model on the MultiMNIST dataset. A set of command-line arguments is provided to allow users to adjust the training configuration.

Some important arguments are described as follows:

- ``dataset``: The type of dataset to use. Options are 'multimnist' (2 tasks) or 'multimnist3' (3 tasks). Default is 'multimnist'.
- ``weighting``: The weighting strategy. Refer to [here](../../LibMTL#supported-algorithms).
- ``arch``: The MTL architecture. Refer to [here](../../LibMTL#supported-algorithms).
- ``gpu_id``: The id of gpu. The default value is '0'.
- ``seed``: The random seed for reproducibility. The default value is 0.
- ``scheduler``: The type of the learning rate scheduler. We recommend to use 'step' here.
- ``optim``: The type of the optimizer. We recommend to use 'adam' here.
- ``dataset_path``: The path where the MultiMNIST dataset will be generated.
- ``train_bs``: The batch size of training data. The default value is 64.
- ``test_bs``: The batch size of test data. The default value is 64.
- ``epochs``: The number of training epochs. The default value is 200.
- ``mode``: The mode to run the script in. Options are 'train' or 'test'.

The complete command-line arguments and their descriptions can be found by running the following command.

```shell
python main.py -h
```

If you understand those command-line arguments, you can train an MTL model by executing the following command.

```shell
python main.py --dataset multimnist --weighting WEIGHTING --arch ARCH --dataset_path PATH/multimnist --gpu_id GPU_ID --scheduler step --mode train --save_path PATH
```

For MultiMNIST3D, use:

```shell
python main.py --dataset multimnist3 --weighting WEIGHTING --arch ARCH --dataset_path PATH/multimnist3 --gpu_id GPU_ID --scheduler step --mode train --save_path PATH
```

You can test the trained MTL model by running the following command.

```shell
python main.py --dataset multimnist --weighting WEIGHTING --arch ARCH --dataset_path PATH/multimnist --gpu_id GPU_ID --scheduler step --mode test --load_path PATH
```

For MultiMNIST3D, use:

```shell
python main.py --dataset multimnist3 --weighting WEIGHTING --arch ARCH --dataset_path PATH/multimnist3 --gpu_id GPU_ID --scheduler step --mode test --load_path PATH
```

### References

<a id="1">[1]</a> Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In *Proceedings of the IEEE*, 1998.

<a id="2">[2]</a> Nikolaos Dimitriadis, Pascal Frossard, and François Fleuret. Pareto manifold learning: Tackling multiple tasks via ensembles of single-task models. In *International Conference on Machine Learning*, 2023. 